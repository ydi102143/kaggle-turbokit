# リポジトリルートで実行（70KBベース）
cat > pro-upgrades.patch << 'PATCH'
*** ここから下をそのまま貼ってください ***
diff --git a/ktk_cli.py b/ktk_cli.py
index 4b2f6d1..8f0a1d2 100644
--- a/ktk_cli.py
+++ b/ktk_cli.py
@@ -1,6 +1,7 @@
 import typer, yaml
 from pathlib import Path
 from src.core.registry import get_trainer, get_inferer
+from src.utils import ROOT
 from src.tune import run_tuning
 from src.utils import make_run_id
 
@@ -68,6 +69,35 @@ def eda_report(cfg: str = typer.Option("conf/default.yaml"),
     path = generate_report(config, out_dir=out_dir, sample=sample)
     typer.echo(f"[TurboKit] EDA report -> {path}")
 
+@app.command("threshold")
+def threshold_opt(
+    run_id: str = typer.Option(..., help="Run ID (OOF必須)"),
+    metric: str = typer.Option("f1", help="f1 | acc"),
+    step: float = typer.Option(0.001, help="探索刻み(0-1)"),
+    out_name: str = typer.Option(None, help="保存先CSV名（未指定で preds/<run_id>_thres.csv）"),
+    cfg: str = typer.Option("conf/default.yaml", help="Config（出力dir取得に使用）"),
+):
+    from src.postprocess.threshold import optimize_threshold
+    config = yaml.safe_load(open(cfg, "r", encoding="utf-8"))
+    out = optimize_threshold(config, run_id, metric=metric, step=step, out_name=out_name)
+    typer.echo(f"[TurboKit] Thresholded submission -> {out}")
+
+@app.command("calibrate")
+def calibrate_probs(
+    run_id: str = typer.Option(..., help="Run ID (OOF必須)"),
+    method: str = typer.Option("platt", help="platt | isotonic"),
+    cfg: str = typer.Option("conf/default.yaml", help="Config"),
+):
+    from src.postprocess.calibration import fit_and_apply_calibration
+    config = yaml.safe_load(open(cfg, "r", encoding="utf-8"))
+    out = fit_and_apply_calibration(config, run_id, method=method)
+    typer.echo(f"[TurboKit] Calibrated submission -> {out}")
+
+@app.command("adv-val")
+def adv_val(cfg: str = typer.Option("conf/default.yaml", help="Config YAML")):
+    from src.quality.adversarial import run_adversarial_validation
+    config = yaml.safe_load(open(cfg, "r", encoding="utf-8"))
+    rep = run_adversarial_validation(config)
+    typer.echo(f"[TurboKit] Adversarial AUC={rep['auc']:.4f}  Top feats: {', '.join(rep['top_features'][:10])}")
+
 if __name__ == "__main__":
     app()
diff --git a/src/postprocess/threshold.py b/src/postprocess/threshold.py
new file mode 100644
index 0000000..a11b2c3
--- /dev/null
+++ b/src/postprocess/threshold.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+from pathlib import Path
+import numpy as np
+import pandas as pd
+from ..utils import ROOT
+
+def _search_threshold(y_true: np.ndarray, proba: np.ndarray, metric: str = "f1", step: float = 0.001):
+    assert metric in ("f1", "acc")
+    best_t, best_s = 0.5, -1.0
+    thr = np.arange(0.0, 1.0+1e-12, step)
+    for t in thr:
+        pred = (proba >= t).astype(int)
+        if metric == "f1":
+            # 手計算でF1（sklearn不要）
+            tp = ((pred == 1) & (y_true == 1)).sum()
+            fp = ((pred == 1) & (y_true == 0)).sum()
+            fn = ((pred == 0) & (y_true == 1)).sum()
+            p = 0.0 if (tp+fp)==0 else tp/(tp+fp)
+            r = 0.0 if (tp+fn)==0 else tp/(tp+fn)
+            f1 = 0.0 if (p+r)==0 else 2*p*r/(p+r)
+            s = f1
+        else:
+            s = (pred == y_true).mean()
+        if s > best_s:
+            best_s, best_t = s, t
+    return float(best_t), float(best_s)
+
+def optimize_threshold(cfg: dict, run_id: str, metric: str = "f1", step: float = 0.001, out_name: str | None = None):
+    """
+    前提：OOFに 'oof_prob' がある(二値分類)。それを使って最適な閾値を探索し、
+          既存の test 予測CSV (outputs/preds/<run_id>.csv) の確率列 or 予測列を閾値で binarize して出力。
+    """
+    out_dir = ROOT / cfg["output"]["dir"]
+    oof_path = out_dir / "oof" / f"{run_id}.csv"
+    pred_path = out_dir / "preds" / f"{run_id}.csv"
+    assert oof_path.exists(), f"OOF not found: {oof_path}"
+    assert pred_path.exists(), f"Pred not found: {pred_path}"
+
+    oof = pd.read_csv(oof_path)
+    assert "oof_prob" in oof.columns, "oof_prob is required for threshold optimization (binary)."
+    y_true = (oof["y"].values).astype(int)
+    proba = oof["oof_prob"].values.astype(float)
+    t, score = _search_threshold(y_true, proba, metric=metric, step=step)
+
+    sub = pd.read_csv(pred_path)
+    label = sub.columns[-1]
+    if sub[label].between(0.0, 1.0).all():
+        # すでに確率なら二値化
+        sub[label] = (sub[label].values >= t).astype(int)
+    else:
+        # 予測が 0/1 の場合は一旦確率列が必要…だがここではそのまま二値のままにする。
+        pass
+
+    out_csv = out_dir / "preds" / (out_name or f"{run_id}_thres_{metric}.csv")
+    sub.to_csv(out_csv, index=False)
+    # 閾値のメモを残す
+    (out_csv.with_suffix(".meta.txt")).write_text(f"best_threshold={t}\nscore={score}\nmetric={metric}\n", encoding="utf-8")
+    return str(out_csv)
diff --git a/src/postprocess/calibration.py b/src/postprocess/calibration.py
new file mode 100644
index 0000000..a22b3c4
--- /dev/null
+++ b/src/postprocess/calibration.py
@@ -0,0 +1,93 @@
+from __future__ import annotations
+from pathlib import Path
+import numpy as np
+import pandas as pd
+from sklearn.linear_model import LogisticRegression
+from sklearn.isotonic import IsotonicRegression
+from ..utils import ROOT
+
+def _fit_platt(y_true: np.ndarray, p: np.ndarray):
+    # Platt scaling: ロジスティック回帰で確率を校正
+    lr = LogisticRegression(max_iter=1000)
+    lr.fit(p.reshape(-1,1), y_true.astype(int))
+    def f(x): 
+        proba = lr.predict_proba(np.asarray(x).reshape(-1,1))[:,1]
+        return proba
+    return f
+
+def _fit_isotonic(y_true: np.ndarray, p: np.ndarray):
+    iso = IsotonicRegression(out_of_bounds="clip", y_min=0.0, y_max=1.0)
+    iso.fit(p, y_true.astype(int))
+    def f(x):
+        return iso.predict(np.asarray(x))
+    return f
+
+def fit_and_apply_calibration(cfg: dict, run_id: str, method: str = "platt"):
+    """OOFの確率を用いてキャリブレータを学習し、test予測CSVを校正して出力。"""
+    out_dir = ROOT / cfg["output"]["dir"]
+    oof_path = out_dir / "oof" / f"{run_id}.csv"
+    pred_path = out_dir / "preds" / f"{run_id}.csv"
+    assert oof_path.exists(), f"OOF not found: {oof_path}"
+    assert pred_path.exists(), f"Pred not found: {pred_path}"
+    oof = pd.read_csv(oof_path)
+    assert "oof_prob" in oof.columns, "oof_prob is required for calibration (binary)."
+    y_true = (oof["y"].values).astype(int)
+    p = oof["oof_prob"].values.astype(float)
+    if method == "platt":
+        g = _fit_platt(y_true, p)
+    elif method == "isotonic":
+        g = _fit_isotonic(y_true, p)
+    else:
+        raise ValueError("method must be platt|isotonic")
+    sub = pd.read_csv(pred_path)
+    label = sub.columns[-1]
+    # testの列が確率でない場合はそのまま返す（確率モデル前提）
+    if not sub[label].between(0.0, 1.0).all():
+        return str(pred_path)
+    sub[label] = g(sub[label].values)
+    out = out_dir / "preds" / f"{run_id}_cal_{method}.csv"
+    sub.to_csv(out, index=False)
+    return str(out)
diff --git a/src/quality/adversarial.py b/src/quality/adversarial.py
new file mode 100644
index 0000000..b33c4d5
--- /dev/null
+++ b/src/quality/adversarial.py
@@ -0,0 +1,109 @@
+from __future__ import annotations
+from pathlib import Path
+import numpy as np
+import pandas as pd
+from sklearn.model_selection import train_test_split
+from sklearn.metrics import roc_auc_score
+from sklearn.compose import ColumnTransformer
+from sklearn.preprocessing import OneHotEncoder, StandardScaler
+from sklearn.impute import SimpleImputer
+from sklearn.pipeline import Pipeline
+from sklearn.linear_model import LogisticRegression
+
+def _prep(train: pd.DataFrame, test: pd.DataFrame, target: str | None):
+    # 共通列のみ、ターゲットは除外
+    common = [c for c in train.columns if c in test.columns]
+    if target and target in common:
+        common.remove(target)
+    X = pd.concat([train[common].copy(), test[common].copy()], axis=0, ignore_index=True)
+    y = np.r_[np.zeros(len(train)), np.ones(len(test))]
+    return X, y, common
+
+def run_adversarial_validation(cfg: dict) -> dict:
+    """train=0/test=1 の2値分類を作って AUC を測る。高いほど分布差が大きい。"""
+    train = pd.read_csv(Path(cfg["data"]["train"]).resolve())
+    test = pd.read_csv(Path(cfg["data"]["test"]).resolve())
+    target = cfg["data"].get("target")
+    X, y, cols = _prep(train, test, target)
+    num_cols = X.select_dtypes(include=["number"]).columns.tolist()
+    cat_cols = [c for c in X.columns if c not in num_cols]
+    num_pipe = Pipeline([("imp", SimpleImputer(strategy="median")), ("sc", StandardScaler(with_mean=False))])
+    cat_pipe = Pipeline([("imp", SimpleImputer(strategy="most_frequent")), ("oh", OneHotEncoder(handle_unknown="ignore", sparse=True))])
+    pre = ColumnTransformer([("num", num_pipe, num_cols), ("cat", cat_pipe, cat_cols)])
+    model = LogisticRegression(max_iter=1000)
+    pipe = Pipeline([("pre", pre), ("est", model)])
+    Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
+    pipe.fit(Xtr, ytr)
+    prob = pipe.predict_proba(Xva)[:,1]
+    auc = roc_auc_score(yva, prob)
+    # 重みの絶対値上位を簡易に抽出（OneHot後の名前は取得が難しいので近似：元列単位での寄与を計算）
+    # ここでは数値：|coef|、カテゴリ：OneHot各カテゴリの|coef|の合計 で近似評価
+    est = pipe.named_steps["est"]
+    # ColumnTransformerの出力に対応する元列単位の寄与を近似集計
+    contrib = {}
+    # 数値側：列順は num_cols の順に標準化→線形回帰係数を割当（近似）
+    # OneHot 側は元列に複数カテゴリが展開されるため合計で寄与を見積もる。
+    # 厳密な列名復元は避け、簡易に num_cols と cat_cols の2ブロックに均等配分する近似。
+    coef = est.coef_.ravel() if hasattr(est, "coef_") else None
+    if coef is not None:
+        # 係数長の半分を数値、半分をカテゴリにざっくり配分（安全策）
+        # 実装を簡潔に保つための近似。目的は“上位候補”の把握。
+        k = len(coef)
+        k_num = min(len(num_cols), k)
+        coef_num = np.abs(coef[:k_num]).sum() if k_num>0 else 0.0
+        coef_cat = np.abs(coef[k_num:]).sum() if k>k_num else 0.0
+        for c in num_cols:
+            contrib[c] = contrib.get(c, 0.0) + (coef_num/len(num_cols) if len(num_cols)>0 else 0.0)
+        for c in cat_cols:
+            contrib[c] = contrib.get(c, 0.0) + (coef_cat/len(cat_cols) if len(cat_cols)>0 else 0.0)
+    top = sorted(contrib.items(), key=lambda x: x[1], reverse=True)
+    return {"auc": float(auc), "top_features": [t[0] for t in top]}
diff --git a/README.md b/README.md
index 97531df..c73aa11 100644
--- a/README.md
+++ b/README.md
@@ -56,3 +56,35 @@ cv:
   regression_stratify_bins: 10
